import tensorflow as tf
import os

# Define the file path and column names
repository = "DanielMarkarov/Cancer-Predictor"
file_path = '/main/Breast_Cancer_Wisconsin_Dataset.csv'
link = f"https://raw.Githubusercontent.com/{repository}/{file_path}"

column_names = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',
                'compactness_mean', 'concavity_mean', 'concave_points_mean', 'symmetry', 'fractal_dimension_mean',
                'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se',
                'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst',
                'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave_points_worst',
                'symmetry_worst', 'fractal_dimension_worst']

# Define data types for each column
data_types = [tf.int32, tf.string] + [tf.float32] * 30

# Define the batch size
batch_size = 569

# Create a dataset object using CsvDataset
dataset = tf.data.experimental.CsvDataset(link, record_defaults=data_types, header=True, select_cols=list(range(len(column_names))))
print("dataset", dataset)

# Perform any required preprocessing steps
# dataset = dataset.map(preprocess_function)

# Shuffle and batch the dataset
dataset = dataset.shuffle(buffer_size=10000).batch(batch_size)

# Iterate over the dataset
for data in dataset:
    id_values, diagnosis_values, *feature_values = data
    # Use the data for training or further processing
    # ...
